{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "069fb884",
   "metadata": {},
   "source": [
    "### Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb69bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym #this is the open ai gym environment\n",
    "from stable_baselines3 import PPO #one of the algorithm that stable baselines3 have\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv #we can create the dummt env to train multiple agents in different env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy #this is to evalute our model using single line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42261c01",
   "metadata": {},
   "source": [
    "### Environment Testing with Random Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edc26170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining name of env\n",
    "env_name = \"CartPole-v0\"\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b1aa820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Score 15.0\n",
      "Episode: 2 Score 19.0\n",
      "Episode: 3 Score 21.0\n",
      "Episode: 4 Score 20.0\n",
      "Episode: 5 Score 12.0\n",
      "Episode: 6 Score 15.0\n",
      "Episode: 7 Score 14.0\n",
      "Episode: 8 Score 11.0\n",
      "Episode: 9 Score 14.0\n",
      "Episode: 10 Score 32.0\n",
      "Episode: 11 Score 20.0\n",
      "Episode: 12 Score 31.0\n",
      "Episode: 13 Score 28.0\n",
      "Episode: 14 Score 15.0\n",
      "Episode: 15 Score 11.0\n",
      "Episode: 16 Score 12.0\n",
      "Episode: 17 Score 28.0\n",
      "Episode: 18 Score 20.0\n",
      "Episode: 19 Score 12.0\n",
      "Episode: 20 Score 22.0\n",
      "Episode: 21 Score 69.0\n",
      "Episode: 22 Score 19.0\n",
      "Episode: 23 Score 15.0\n",
      "Episode: 24 Score 11.0\n",
      "Episode: 25 Score 20.0\n",
      "Episode: 26 Score 9.0\n",
      "Episode: 27 Score 18.0\n",
      "Episode: 28 Score 15.0\n",
      "Episode: 29 Score 21.0\n",
      "Episode: 30 Score 11.0\n",
      "Episode: 31 Score 17.0\n",
      "Episode: 32 Score 51.0\n",
      "Episode: 33 Score 13.0\n",
      "Episode: 34 Score 33.0\n",
      "Episode: 35 Score 29.0\n",
      "Episode: 36 Score 18.0\n",
      "Episode: 37 Score 11.0\n",
      "Episode: 38 Score 25.0\n",
      "Episode: 39 Score 17.0\n",
      "Episode: 40 Score 15.0\n",
      "Episode: 41 Score 17.0\n",
      "Episode: 42 Score 22.0\n",
      "Episode: 43 Score 25.0\n",
      "Episode: 44 Score 16.0\n",
      "Episode: 45 Score 52.0\n",
      "Episode: 46 Score 28.0\n",
      "Episode: 47 Score 16.0\n",
      "Episode: 48 Score 19.0\n",
      "Episode: 49 Score 10.0\n",
      "Episode: 50 Score 18.0\n"
     ]
    }
   ],
   "source": [
    "#testing the env with some random actions\n",
    "#here player is called as agent\n",
    "#here the actions are moving left or right that agent takes(random actions)\n",
    "\n",
    "\n",
    "for episode in range(1, 51): #taking 10 episodes to iterate\n",
    "    score = 0 #reward points\n",
    "    state = env.reset() #reseting it as a new game\n",
    "    done = False #if episode is completed it return True\n",
    "    #until that we use while loop\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample() #in env.action_space they are 2 discrete values either 0 or 1 sample means it is actually random action it will give some number 0 or 1 that moving left or right\n",
    "        #after getting action we have to make that action in the env that is in the game\n",
    "        \n",
    "        n_state, reward, done, info = env.step(action) #it will take corresponding action and it returns 4 variables\n",
    "        score += reward #if we have any reward we are adding it tothe score\n",
    "        \n",
    "    print(\"Episode:\", episode, \"Score\", score)\n",
    "#env.close() #closing the env\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d535f408",
   "metadata": {},
   "outputs": [],
   "source": [
    "#69 is the highest score but we need to improve the score upto 200.The above are only random actions here we are not training any agent.\n",
    "#If the pole is inthe middle the score will be high if the pole is fallen the episode will be stopped.\n",
    "#After training the model it try improve the score it the objective inthe reinforcement learning, its tries to increase the award."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e8c61f",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c56b934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "#again we to create the env\n",
    "env = gym.make(env_name)\n",
    "\n",
    "#to vectorize for training\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "#intiallizing the model\n",
    "model = PPO('MlpPolicy', env, verbose=1) #here we have to mention some policy. Its nothing but set of instructions that means how to take input and produce the output\n",
    "#MlpPolicy will take the current state and it will give appropriate action that how this policy works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f43d788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1418 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Viewer.__del__ at 0x0000019BBA726E60>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Krishna Vamshi\\anaconda3\\envs\\reinforcement_env\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 185, in __del__\n",
      "    self.close()\n",
      "  File \"C:\\Users\\Krishna Vamshi\\anaconda3\\envs\\reinforcement_env\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 101, in close\n",
      "    self.window.close()\n",
      "  File \"C:\\Users\\Krishna Vamshi\\anaconda3\\envs\\reinforcement_env\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\", line 336, in close\n",
      "    super(Win32Window, self).close()\n",
      "  File \"C:\\Users\\Krishna Vamshi\\anaconda3\\envs\\reinforcement_env\\lib\\site-packages\\pyglet\\window\\__init__.py\", line 857, in close\n",
      "    app.windows.remove(self)\n",
      "  File \"C:\\Users\\Krishna Vamshi\\anaconda3\\envs\\reinforcement_env\\lib\\_weakrefset.py\", line 114, in remove\n",
      "    self.data.remove(ref(item))\n",
      "KeyError: <weakref at 0x0000019BBA78C3B0; to 'Win32Window' at 0x0000019BB9FC4970>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 945         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009217174 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.00712    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.18        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    value_loss           | 46.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 902         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009732593 |\n",
      "|    clip_fraction        | 0.0683      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.667      |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20          |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    value_loss           | 38.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 857         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009059958 |\n",
      "|    clip_fraction        | 0.0817      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.633      |\n",
      "|    explained_variance   | 0.191       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.7        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    value_loss           | 56          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 771         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007193117 |\n",
      "|    clip_fraction        | 0.0494      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.618      |\n",
      "|    explained_variance   | 0.283       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.9        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    value_loss           | 62.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 783          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074836314 |\n",
      "|    clip_fraction        | 0.0792       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.609       |\n",
      "|    explained_variance   | 0.416        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.9         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0146      |\n",
      "|    value_loss           | 60.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 732          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037332498 |\n",
      "|    clip_fraction        | 0.0212       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.579       |\n",
      "|    explained_variance   | 0.219        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 35.1         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00623     |\n",
      "|    value_loss           | 76.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 702          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054803677 |\n",
      "|    clip_fraction        | 0.0443       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.587       |\n",
      "|    explained_variance   | 0.902        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.61         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00575     |\n",
      "|    value_loss           | 23.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 702          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056840144 |\n",
      "|    clip_fraction        | 0.0528       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.583       |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.76         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.0111      |\n",
      "|    value_loss           | 19.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 677         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006461558 |\n",
      "|    clip_fraction        | 0.0376      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.569      |\n",
      "|    explained_variance   | 0.583       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.9        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00487    |\n",
      "|    value_loss           | 40          |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x19bb9f9f8b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model training\n",
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6a1d207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "#model.save('ppo model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61603ab2",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d01d454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krishna Vamshi\\anaconda3\\envs\\reinforcement_env\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200.0, 0.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb9b70f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#so we are training will less time stamps we can some oscillations in the pole but if we train with more we takeoff the oscillations so we need to train more. But as for now its fine\n",
    "#the above score is the average score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7c8a2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#closing the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67c6da2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krishna Vamshi\\anaconda3\\envs\\reinforcement_env\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:406: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Score [200.]\n",
      "Episode: 2 Score [200.]\n",
      "Episode: 3 Score [200.]\n",
      "Episode: 4 Score [200.]\n",
      "Episode: 5 Score [200.]\n",
      "Episode: 6 Score [200.]\n",
      "Episode: 7 Score [200.]\n",
      "Episode: 8 Score [200.]\n",
      "Episode: 9 Score [200.]\n",
      "Episode: 10 Score [200.]\n",
      "Episode: 11 Score [200.]\n",
      "Episode: 12 Score [200.]\n",
      "Episode: 13 Score [200.]\n",
      "Episode: 14 Score [200.]\n",
      "Episode: 15 Score [200.]\n",
      "Episode: 16 Score [200.]\n",
      "Episode: 17 Score [200.]\n",
      "Episode: 18 Score [200.]\n",
      "Episode: 19 Score [200.]\n",
      "Episode: 20 Score [200.]\n",
      "Episode: 21 Score [200.]\n",
      "Episode: 22 Score [200.]\n",
      "Episode: 23 Score [200.]\n",
      "Episode: 24 Score [200.]\n",
      "Episode: 25 Score [200.]\n",
      "Episode: 26 Score [200.]\n",
      "Episode: 27 Score [200.]\n",
      "Episode: 28 Score [200.]\n",
      "Episode: 29 Score [136.]\n",
      "Episode: 30 Score [200.]\n",
      "Episode: 31 Score [200.]\n",
      "Episode: 32 Score [200.]\n",
      "Episode: 33 Score [200.]\n",
      "Episode: 34 Score [166.]\n",
      "Episode: 35 Score [200.]\n",
      "Episode: 36 Score [200.]\n",
      "Episode: 37 Score [200.]\n",
      "Episode: 38 Score [200.]\n",
      "Episode: 39 Score [85.]\n",
      "Episode: 40 Score [200.]\n",
      "Episode: 41 Score [200.]\n",
      "Episode: 42 Score [200.]\n",
      "Episode: 43 Score [200.]\n",
      "Episode: 44 Score [200.]\n",
      "Episode: 45 Score [200.]\n",
      "Episode: 46 Score [200.]\n",
      "Episode: 47 Score [200.]\n",
      "Episode: 48 Score [200.]\n",
      "Episode: 49 Score [200.]\n",
      "Episode: 50 Score [200.]\n"
     ]
    }
   ],
   "source": [
    "#alternative way for the above process.The process is for random actions \n",
    "#now to make the action based on the model we have todo few changes\n",
    "for episode in range(1, 51):\n",
    "    score = 0\n",
    "    obs = env.reset() #instead of state we call it observation\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        env.render(mode = \"rgb_array\") #after rendering the env and render \"rgb_array\" return rendererd image as numpy array\n",
    "        action, _ = model.predict(obs) #here it will creates some dummy values\n",
    "        #based on the previous observations it will try to predict the next action\n",
    "        \n",
    "        obs, reward, done, info = env.step(action) #chaning n_state to observation and taking action into env\n",
    "        score += reward\n",
    "    \n",
    "    print(\"Episode:\", episode, \"Score\", score)\n",
    "env.close() #closing the env\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f5b80fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01173841,  0.02228636, -0.02198895,  0.04008972]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#observation state look like\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "522b65c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#so those are the observation state each value represents some attributes\n",
    "#it will represents the position ofthe card pole and velocity like that it have some attributes\n",
    "#after taking each action the observation changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3560e1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02633002, -0.01649812, -0.04067033,  0.03953071]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if we say env reset\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "668b91b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the above the intial observations\n",
    "#so after some taking some actions and some steps it will have new states or values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60f10fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1\n",
    "\n",
    "totals = []\n",
    "for episode in range(1000):\n",
    "    episode_rewards = 0\n",
    "    obs = env.reset()\n",
    "    for step in range(400):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    totals.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a7430b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42.382, 8.999781997359714, 24.0, 68.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(totals), np.std(totals), np.min(totals), np.max(totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d60c3ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42.008, 8.794540124418104, 24.0, 72.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(totals), np.std(totals), np.min(totals), np.max(totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c03fd1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I tried with 500 and 1000 episodes repectively but not managed to keep pole striaght for more 68.0 or 72.0 consecutive steps.so we need to keep train more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f736925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
